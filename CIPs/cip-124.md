---
cip: 124
title: Recon
author: Aaron Goldman (@aarongoldman) <aaron@3box.io>, Joel Thorstensson <joel@3box.io>
discussions-to: https://forum.ceramic.network/t/cip-124-recon-tip-synchronization-protocol/1144
status: Draft
category: Networking
created: 2023-01-18
edited: 2023-05-24
---
## Simple Summary

<!--Provide a simplified and layman-accessible explanation of the CIP.-->
Recon is a sets reconciliation protocol to be used for synchronization of stream tips in ceramic network.
Stream Sets bundle a number of streams together so the ceramic node with a common interest in those streams can synchronize efficiently.
Once a set of streams is bundled into a stream set Recon can be used to sync ranges within those sets.


## Abstract
<!--A short (~200 word) description of the technical issue being addressed.-->
Stream Sets are bundles of streams that can be gossiped about as a group or in sub-ranges. In order to make ranges possible over a set of streams we need to define an order for the ranges to be over. e.g. `ceramic/<network>/<sep>/<EventID>` by assigning a recon key to each event we can define a range over the lexicographic sort of the sort keys. Now a node can advertise a have/want over a range `(first event ID, range_hash, last event ID)` If the node receiving the advertisements has the same set of events, then they will have the same hash and are in sync. If a node needs to advertise a new event it can send `(first_event_ID, hash, new_event_ID, hash, last_event_ID)` This tells any receiving node not only the new event ID but asserts all events in the range. If the receiving node has any additional or missing events in this range it is detected and can be synchronized.

New nodes interested in a range of a stream set can be discovered in two ways. First if a node receives a have/want advertisement from a node it can add that node to its list of peers. Second nodes should advertise their wants to other known nodes in the network.


## Motivation
<!--Motivation is critical for CIPs that want to change the Ceramic protocol. It should clearly explain why the existing protocol specification is inadequate to address the problem that the CIP solves. CIP submissions without sufficient motivation may be rejected outright.-->
Currently ceramic nodes broadcast updates to streams to every node in the network. This requires nodes to do work processing messages that they don’t care about. At the same time if a node missed the broadcast, it would not detect the missing stream unless it hears a later update or performs historical sync via the CAS ledger.

Recon could provide low to no overhead for nodes not synchronizing a given range of events, while retaining a high probability of getting the **latest** events from a stream shortly after any node has the events, and no need for remote connections at query time.

By pulling stream updates out of the main network channel into a stream set the nodes interested in those streams can synchronies with each other without putting load on uninterested nodes in the network. 

These Stream Sets will likely be defined by a set of models that should be included in the Stream Set. They could also be defined by a set of models and controllers but the large and changing number of controllers requires more thought and is not proposed here. They could even be defined be an arbitrary function that filters streams for membership in/out of the set. e.g. Failed some spam filter.

A stream also could potentially be included in more than one stream set if for example there are apps that want to sync every event across the entire network, they may want to form a stream set for all events that overlaps the narrower stream sets. We will start with one stream set that orders all the streams into a single stream set.

The introduction of stream sets should lower the burden of running a node that is not interested in a stream set. This is due to sending the synchronization connections only to nodes that advertise an interest in a range. Stream set synchronizations could speed up the process of syncing all the historical events that are within a range by by detecting and backfilling missed updates with every synchronization update.

A secondary goal of stream sets is to give a structure for sharding a stream set across ceramic nodes. By supporting an ability to synchronize only a sub-range of the stream set the burden of storing, indexing, and retrieving streams can be sharded among ceramic nodes.

e.g.
```
Node1: [
  "ceramic/<network>/<sep>/!", 
  "ceramic/<network>/<sep>/i"
]
Node2: [
  `ceramic/<network>/<sep>/i`,
  `ceramic/<network>/<sep>/~`
]
```
> note: The keys are base36 /`[0-9a-z]`/ so "`!`" is less then all keys and "`~`" is grater then all keys.

Ceramic nodes also need to have a way to find the other nodes interested in the stream set so that they can synchronize with them.
Recon will rely on gossiping about nodes existence and interests. When two nodes connect they can share lists of known nodes and their interests.

By gossiping about ranges of streams ceramic nodes that are arbitrarily out of sync can synchronize all events of mutual interest. Nodes that are in sync or nearly in sync can send each other very little bandwidth. Nodes can avoid sending stream event announcements to nodes that have no interest in the stream ranges.

## Specification
<!--The technical specification should describe the syntax and semantics of any new feature.-->

### Stream Set

A stream set is defined by two values, `separator-key` and `separator-value`. A stream is considered to be part of a stream set its InitEvent contains a header field that maps to the *separator-key* and the value of this field is equal to the *separator-value*. For example, stream set containing all *Model Instance Documents* in the *Model* stream `kjzl6hvfrbw6c82mkud4qs38zl4hd03ifoyg2ksvfjkhuxebfzh3ef89vwvtvrr` would be defined as:

```js
separator-key = 'model'
separator-value = kjzl6hvfrbw6c82mkud4qs38zl4hd03ifoyg2ksvfjkhuxebfzh3ef89vwvtvrr
```

### EventIds

An eventId is a special type of [StreamId](https://cips.ceramic.network/CIPs/cip-59) that encodes information about an event and its order in a stream set. It is constructed as follows,

```xml
eventId = concatBytes(
  varint(0xce),
  varint(0x71),
  eventIdBytes
)
```

Where `0xce` is the streamid multicode, `0x71` is the streamtype code for an eventId (note that this is also the multicode for dag-cbor). The `eventIdBytes` are created as follows,

```js
eventIdBytes = EncodeDagCbor([ // ordered list not dict
  concatBytes(
    varint(networkId),
    last16Bytes(separator-value),
    last16Bytes(hash-sha256(stream-controller-did)),
    last8Bytes(init-event-cid)
  ),
  prevTimestamp,
  eventHeigh,
  eventCID
])
```

Where:

* `networkId` is a number as defined in the [networkIds table](../tables/networkIds.csv)

* `separator-value` is based on a user provided value for *separator-key* and *separator-value*
* `stream-controller-did` is the controller DID of the stream this event belongs to
* `init-event-cid` is the CID of the first, e.g. InitEvent, of the stream this event belongs to
* `prevTimestamp` is the unix timestamp in most recent TimeEvent that came before this event. Note that for InitEvents this value is 0, and for TimeEvents this value is the timestamp of the TimeEvent that came before it.
* `eventHeight` is the "height" of the event since the most recent TimeEvent. For InitEvents and TimeEvents this value is 0.
* `eventCID` the CID of the event itself
* `last16Bytes` and `last8Bytes` takes the last N bytes of the input and prepends with zeros if the input is shorter

### Recon Message

The Recon protocol uses a binary string as a message for communication. This message is constructed in the following way,

```
(EventId (Ahash EventId)+ )+
```

Every recon message starts and ends with an eventId and in between every eventId there is an ahash (see [Appendix A](#appendix-a-associative-fash-function)) of all of the eventIds in-between. For efficiency the ahash can be represented using a b-tree under the hood (see [Appendix B](#appendix-b-btree-b-hash-trees))), but this is not a strict requirement. The message can be a binary string because both eventIds and ahash use multicodes so the parser can know when the end of any particular eventId or ahash has been reached.

### Stream Set Ranges

With the definition of eventIds above we get an absolute ordering of events. We can now define subsets of the total range of all eventIds by defining a start and a stop eventId. 

For example, to construct the range of all streams defined by the *Model* `kjzl6hvfrbw6c82mkud4qs38zl4hd03ifoyg2ksvfjkhuxebfzh3ef89vwvtvrr`, we would construct the start and stop eventIds as follows:

```js
startEventIdBytes = EncodeDagCbor([
  concatBytes(
    0x00,
    last16Bytes(kjzl6hvfrbw6c82mkud4qs38zl4hd03ifoyg2ksvfjkhuxebfzh3ef89vwvtvrr),
    last16Bytes(repeat16(0x00)),
    last8Bytes(repeat8(0x00))
  )
])
endEventIdBytes = EncodeDagCbor([
  concatBytes(
    0x00,
    last16Bytes(kjzl6hvfrbw6c82mkud4qs38zl4hd03ifoyg2ksvfjkhuxebfzh3ef89vwvtvrr),
    last16Bytes(repeat16(0xff)),
    last8Bytes(repeat8(0xff))
  )
])
```

Given this it should be simple to see how we could split the range into subsets as well.

### Interactive Sync Algorithm

The following synchronization protocol inspired by "Range-Based Set Reconciliation and Authenticated Set Representations" [[arXiv:2212.13567](https://doi.org/10.48550/arXiv.2212.13567)]. It will be implemented as a [Libp2p protocol](https://docs.libp2p.io/concepts/fundamentals/protocols/) with the following protocol id:

```
/ceramic/recon/1.0.0
```

Let's look at an example. to keep it small we substitute binary eventIds with short strings like `ape` and `bee`. 

<img src="../assets/cip-124/ring.png" alt="Recon ring" width="60%"/>

The example will use the following format:

```
you:  [ your initial keys ]
they: [ their initial keys ]
    -> ( request ) [ their keys after processing request]
    <- ( response ) [ your keys after processing response]
```
The full example can be observed below,

```
you:  [ape,eel,fox,gnu]
they: [bee,cat,doe,eel,fox,hog]
    -> (ape, h(eel,fox), gnu) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, h(bee,cat), doe, h(eel,fox), gnu, 0, hog) [ape,doe,eel,fox,gnu,hog]
    -> (ape, 0, doe, h(eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, 0, bee, 0, cat, h(doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    -> (ape, h(bee,cat,doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, h(bee,cat,doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
```

You initiate a synchronization by sending a hash covering the entire range.

```
    -> (ape, h(eel,fox), gnu)
```

They don't have the same hash so they split the range near the middle at `"doe"`
They also have a key after your last key so they send `"hog"`

```
 <- (ape, h(bee,cat), doe, h(eel,fox), gnu, 0, hog)
```

You have nothing from `"ape"` to `"doe"` so send `0`.
Your hash from `"doe"` to `"gnu"` match.
Your hash from `"gnu"` to `"hog"` match.
You merge the match streak from `"doe"` to `"hog"`.

```
 -> (ape, 0, doe, h(eel,fox,gnu), hog)
```

You sent 0 from `"ape"` to `"doe"` so they send all keys in that range.
The rest match

```
<- (ape, 0, bee, 0, cat, h(doe,eel,fox,gnu), hog)
```

with the new key all ranges merge to `"ape"`, `"hog"`
they also merge to `"ape"`, `"hog"` nothing to send we are in sync

```
    -> (ape, h(bee,cat,doe,eel,fox,gnu), hog)
    <- (ape, h(bee,cat,doe,eel,fox,gnu), hog)![ring](../assets/cip-124/ring.png)
```

#### Random Peer Sync Order

When a new event is received, either from the user or from a peer using recon, the eventId of this event needs to be synchronized to other peers in the network. This is achieved by simply sending a new recon message with our latest range to a set of random peers. While synchronizing we want to sync with nearby nodes with higher probability since this is lower cost. However, sometimes we also want to sync with the far away nodes so that the whole network is kept synchronized. By keeping a list of known peers with overlapping interest, while tracking the latency to these peers, we can choose a peer at random proportional to the overlapping interests and inversely proportional to the latency. 

<img src="../assets/cip-124/gossip.gif" alt="Random gossip" width="60%"/>



Following this algorithm, the number of nodes with the event will grow exponentially until it saturates the network.

#### Interest Gossip

Besides sending a recon message when a new event is received, we also want to gossip to other peers about the ranges we are interested in. We can do so by simply sending a recon message for every range we are interested in to all peers we know about at a certain interval. On the receiving end we would simply keep track of interests of other peers we've heard from in our internal peer list.

### Peer discovery

Besides existing bootstrap nodes, recon leverages the [libp2p kademlia DHT](https://docs.libp2p.io/concepts/discovery-routing/kaddht/) to discover other peers in the network. There are two types rendezvous approaches suggested. Both of them use the eventId format to construct a key to publish provider records to. In both of these approaches peers are expected to periodically republish their records and periodically check for new peers at the rendezvous point.

#### Discover all peers in the same network

We can announce and discover peers in the same network by only including the networkId in the eventId encoding.

```
eventId = concatBytes(
  varint(0xce),
  varint(0x71),
  EncodeDagCbor([ varint(networkId) ])
)
```

#### Discover peers using the same separator-value

We can announce and discover peers in the same network and that are using the same separator-value by including the networkId and the separator-value in the eventId encoding.

```
eventId = concatBytes(
  varint(0xce),
  varint(0x71),
  EncodeDagCbor([
    concatBytes(
      varint(networkId),
      last16Bytes(separator-value)
    )
  ])
)
```

## Rationale
<!--The rationale fleshes out the specification by describing what motivated the design and why particular design decisions were made. It should describe alternate designs that were considered and related work, e.g. how the feature is supported in other languages. The rationale may also provide evidence of consensus within the community, and should discuss important objections or concerns raised during discussion.-->

### Why the properties in eventIds

* Why specific lengths?
* Size overhead?

With about a billon events per year as a target this is hundreds of gigabytes just for the keys without event accounting for the size of the events.

### Motivation for the design

Goals:

- Low to no overhead for peers not subscribed to the stream being queried.
- High probability of getting the ******latest****** tip (e.g. resilience to eclipse attacks etc.)
- Low overhead for the querying peer (e.g. no requirement to connect to and query a massive number of peers)

Ceramic has a model where each ceramic node with an interest in a stream stores the contents of the stream. This means when a stream is created or updated each ceramic node should eventually hear about the new event and update its stream tip store.

Currently this is being done by using a LibP2P PubSub channel. This works for low amounts of traffic while a node is up and listening to the channel. There are two main drawbacks of this method. One that if a node is down and misses an update it will not find out about the stream until the next update. Since the next update may never come nodes need to periodically run Historical Data Sync to discover events that it missed but are successfully anchored. Two that every node needs to process every update. Even if a node is only interested in a few low frequency models or controllers it still needs to be a large enough node to handle all updates and filter for the one that it is interested in.

The LibP2P PubSub channel is meant for data that has only a limited time of interest. It only remembers messages for a short time then expires them. If a node gets very behind in processing the incoming message queue it can re-broadcast messages after the other nodes have forgotten it causing the message to be redelivered to all nodes in the channel. Since the node is way behind it will see its own message later and re-re-broadcast it. This infinitely looping message can slow down other nodes that start re-broadcast messages leading to a network storm. A single slow, or malicious, node can harm the entire network’s ability to use the channel. Event update lost in the storm may not be picked up until Historical Data Sync runs.

By switching to a set reconciliation gossip protocol (Recon) we solve both problems.

One, if a node missed a stream update it will detect that the node it is synchronizing with has an event that it does not. A node could go down for years and when it rejoined the network it would just sync back to the current state.

Two, if a node is only interested in a subset of the streams, it can synchronize only the ranges that it cares about. A single node that is too slow to keep up with the number of streams it is interested in will start a synchronization that will never finish but will only cause load on the node that it is synchronizing with and not cause a network wide disruption.

---

### Alternate designs considered and related work

Hash graph stile gossip

Each node collects the event that occur on that node into an update block. Each time a node receives an update block from a different node it puts the CID of its previous block and the incoming block at the end of the current block to finish that update block and starts a new one. Then send the finished block to a random ceramic node. Since each block has the CID of two earlier blocks if can follow back to the beginning of time.

This was rejected since it did not allow for a node to follow a limited subset of the streams.

Predicated LibP2P PubSub channels

Can we change LibP2P PubSub to only send the events that a node cares about to limit the number of messages a node needs to process.

If we can use a TTE(Time To Expire) as part of the predicate we may be able to solve the network storm problem.

This was rejected because it does not solve the missed messages problem.

We still plan to revisit this in the future

#### **DHT for interest discovery**
In order to discover other ceramic nodes that have overlapping interest
a node could announce itself in the DHT as interested in a model.
the dht key would be derived by `separatorKey` and `separatorValue`

```
key =  are set, use `partition = hash(separatorKey | separatorValue)`
```

---
#### **Stream set sort order**
example lexicographical sort order
```
ceramic/<network>/<partition_key>/<EventID>
```
The `partition_key` would be defined for a specific stream set.

For the default stream set with all streams we could partition on`stream type`, `model`, `controller`, `time` , or something else using a partition key? We could even map multiple dimensions into a single partition key and then partition based on the calculated partition key.
![model controller time cube](../assets/cip-124/cube.png)

#### **What are the sorting orders that make sense?**
- Total time ordering of all anchored events in all streams.
    - `ceramic/<network>/<timeID>/<EventID>`
    - advantages: most divergence is in recent events
    - disadvantages:
      - hard to shard as most load is recent events,
      - hard to follow a stream,
      - hard to follow a controller 
- Sort by StreamID lexicographic
    - `ceramic/<network>/<StreamID>/<EventID>`
    - advantages: simple to shard to StreamID ranges
    - disadvantages: following a model or controller would be many ranges
- Sort by model/StreamID
    - `ceramic/<network>/<model>/<StreamID>/<event height><Event CID>`
    - advantages:
      - following a models is a single range
    - disadvantages:
      - following a controllers is a many ranges

- Sort by model_group/model/stream_group/StreamID/time/event
    - `ceramic/<network>/<model controller>/<model>/<stream controller>/<StreamID>/<event height>/<Event CID>`
    - advantages: models grouped by publisher, streams grouped by publisher, stream events in order
    - disadvantages: long keys

- Z-order (Hilbert Curve order)
    - `ceramic/<network>/< z(model, controller, time) >`
    - advantages: 
      - clustering by model, controller, and time
    - disadvantages:
      - complexity
      - all three dimensions have few ranges but none are single range.
    - notes:
      - [https://youtu.be/YLVkITvF6KU](https://youtu.be/YLVkITvF6KU)
      - [https://en.wikipedia.org/wiki/Z-order_(curve)](https://en.wikipedia.org/wiki/Z-order_(curve))
      - e.g.
        1. hash the model
        2. hash the controler
        3. timestamp
        3. interleave the bytes of the hash M00:C00:T00:M01:C01:T00:...
        4. cluster into chunks e.g. 64MiB files

example

partition_key = model_controller/model/stream_controller/stream/height/event
```
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6hvfrbw6c5sffjlmczg8nmbk8kwu9lmgiqfd9bxi7pxp14u674cuxp09szz/
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6kcym7w8y7hyovnujm2zbxa57z0z0yhmnlsx9qe4gtyurcbg6z2aw967s0d/
0/
bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue
```

```
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6hvfrbw6c5sffjlmczg8nmbk8kwu9lmgiqfd9bxi7pxp14u674cuxp09szz/
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6kcym7w8y7hyovnujm2zbxa57z0z0yhmnlsx9qe4gtyurcbg6z2aw967s0d/
1/
bagcqcerand3n6q246mfo2v7d6i7aacpxlfnfprhyid5rcnej2bawqnlnsogq
```

this would be about 300 chars so we may want to abbreviate
model_controller/model/stream_controller/stream/height/event
`JSwygB/xp09sz/JSwygB/w967s0/0/bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue`
`JSwygB/xp09sz/JSwygB/w967s0/1/bagcqcerand3n6q246mfo2v7d6i7aacpxlfnfprhyid5rcnej2bawqnlnsogq`

Model_controller keeps model published by the same model creator together.
Model keeps all the documents of a model together.
Stream_controller keeps streams published by the same stream creator together.
StreamID keeps the updates to a stream together.
Height keeps the updates to a stream in order.

Since interests are expressed as interest ranges a ceramic node interested in a set of models relevant to an application
will be able to subscribe with a small number of ranges. The number of ranges will be on the order of the number of
model_controllers they are interested in. If application developers publish all the models for their application using
a single controller then the models will be efficient to sync as a group.

## Backwards Compatibility
<!--All CIPs that introduce backwards incompatibilities must include a section describing these incompatibilities and their severity. The CIP must explain how the author proposes to deal with these incompatibilities. CIP submissions without a sufficient backwards compatibility section may be rejected outright.-->
This protocol should run in parallel with the libp2p pubsub channel for tip synchronization with out a problem.

<!--
## Implementation
<!--The implementations must be completed before any CIP is given status "Final", but it need not be completed before the CIP is accepted.-- >
Implementation goes here.
-->

WIP implementation in [rust-ceramic](https://github.com/3box/rust-ceramic/)


## Security Considerations
<!--All CIPs must contain a section that discusses the security implications/considerations relevant to the proposed change. Include information that might be important for security discussions, surfaces risks and can be used throughout the life cycle of the proposal. E.g. include security-relevant design decisions, concerns, important discussions, implementation-specific guidance and pitfalls, an outline of threats and risks and how they are being addressed. CIP submissions missing the "Security Considerations" section will be rejected. An CIP cannot proceed to status "Final" without a Security Considerations discussion deemed sufficient by the reviewers.-->
The associative hash functions are only secure if the node is asked to produce the EventIDs that hash to the SHAs that make up the SumOfShas associative hash.

Recon works when you want to synchronize event that are contiguous in the key ordering.
If a spammer is creating junk events each with there own controller did they will be spread uniformly throw the model.
This implies that if we want to exclude the spam events there will be many holes and the representation will have many ranges. 



## Appendix A: Associative Hash Function



* Explain the hash function
* Explain the registration in the multicodec table



## Appendix B: B#tree (B hash trees)





## Appendix C: Immutable object store

A slight variant of the protocol can be used with a smart client and a dumb object store. Where all the computation takes place on the client and the storage just serves values for keys.

For syncing with an immutable object store we add CIDs to the intermediate nodes of a tree. This would be DAG-JSON or DAG-CBOR

```
(EventID (sub-tree-ahash, sub-tree-CID, EventID)+ )+
```

This protocol can be run against a object store. Instead of the remote deciding which segments to send a pre-built b-tree is stored in the object store. The client fetches the root of the b-tree. If the client has the same associative hash `sub-tree-ahash` for a subtree then it skips that subtree if it differs then it requests the b-tree node with `sub-tree-CID`. This continues recursively until all the events in the stored b-tree are synced down to the client.

The use of synchronization against an object store may improve the experience of syncing a brand new node or a node that has been down for an extended period of time. The long down node could start by synchronizing against a weekly snapshot and only after that sync with live nodes getting the bulk of the dataset from the object store.



## Appendix
### **B#tree (B hash trees)**
e.g. [MST](https://hal.inria.fr/hal-02303490/document) / [Prolly Trees](https://docs.dolthub.com/architecture/storage-engine/prolly-tree)

Advantage:
  - Can be sharded to split among nodes.
  - Can have independent tree structure, fanout, balancing, or leveling.

Disadvantage:
  - A stream set must agree on a sorting order of events.

![b hash tree](../assets/cip-124/b_hash_tree.png)

The B#tree (Bee-Hash-Tree) is a form of B-tree
where the links are hashes rather than pointers.

1. Pointers are CIDs (hashes)
2. iNode is functionally **dependent** on the nodes that **the node points to**.
3. iNode is functionally **independent** of the nodes that **point to the node**.
4. Keys and subtrees are in sorted order within an iNode
5. Level is functionally dependent set of keys, and optionally values


![merkle search tree](../assets/cip-124/merkle_search_tree.png)

The MST uses only the hash of the key to determine the level of the tree where that key is stored.
This will lead to probabilistically log(n) depth and log(n) keys in the per node.
The Prolly tree uses a running hash of the keys from left to right to determine level of the tree.
Sliding a fixed-size window through it, one byte at a time.
This enables the Prolly Tree to better calibrate the node size variance.
e.g. As the node size grows increase the probability of a boundary. 
This will reduce the odds of small nodes and large nodes
in favor of target size nodes.


### **Associative Hash Function (SumOfSha256s)**
If we want to calculate the hash of a range of
EventIDs without needing to visit all of the events
we can use an associative hash function where the grouping
of EventIDs is not relevant to the final hash.

```
h((A, B), C) = h(A, (B, C))
```

e.g. use `sum(sha256(EventID) for EventID in Stream Set)`
each node in the tree can store the associative hash of each
sub tree along with its max and min we only need to recurse into that sub-tree if the range end is in the sub-tree. Since a range has only two ends, start and stop, we can calculate the associative hash by visiting only twice the depth of the tree number of nodes `2*log_b(n)`
where b is the fanout and n is the number of EventIDs.

#### ahash
To get the ahash of a set we start by using SHA256 two convert the set elements to 32 byte hashs.
Next each of the 32 byte hashes are treated as an array of 8 unsigned little endian 32 bit integers.

To add to ahashs we use piecewise addition with all the additions here mod `2^32`. `C = A + B`
```
c[0] = a[0] + b[0]; c[1] = a[1] + b[1]; c[2] = a[2] + b[2]; c[3] = a[3] + b[3];
c[4] = a[4] + b[4]; c[5] = a[5] + b[5]; c[6] = a[6] + b[6]; c[7] = a[7] + b[7];
```
Due to the associativity it doesn't matter the order in witch you add the elements of the set.
This can be done in a loop with an accumulator or as a tree.
If you have a large set distributed across many nodes the hasher can hash and add all element locally and
then send the hashes to one node for final combination.

Little endian 32 bit integers are chosen since x86 and arm CPUs use little endian unsigned integers.
u32 was chosen since it will fit in a JS number and can be calculated in js without on reliance.
big number libraries.

Treating the hash as u8 was rejected since it is less performant and using the xor as the associative add
was rejected since having a value twice will look the same as not havening that element at all.

A b-tree with fanout 3:
![fanout3](../assets/cip-124/b_hash_tree_1.png)

A b-tree with fanout 2:
![fanout2](../assets/cip-124/b_hash_tree_2.png)
## Copyright
Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
