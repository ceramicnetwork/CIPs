---
cip: 124
title: Recon
author: Aaron Goldman (@aarongoldman) <aaron@3box.io>
discussions-to: https://forum.ceramic.network/t/cip-124-recon-tip-synchronization-protocol/1144
status: Draft
category: Standards
type: Networking
created: 2023-01-18
edited: 2023-01-18
---
<!-- STEPS TO SUBMIT A CIP:
1. Complete the header above.
2. Fill in as much content as is appropriate for the status of your CIP.
3. Add Github labels for status, category, and type.-->

<!--[status]: Here is a description of status terms.
- `Idea`: an CIP issue that is incomplete.
- `Draft`: an CIP issue that is complete but undergoing rapid iteration and changes.
- `Last Call`: an CIP issue that is stable and ready for final review by the community.
- `Pending`: an CIP that has been submitted as a PR or merged but not finalized.-->

## Simple Summary
<!--Provide a simplified and layman-accessible explanation of the CIP.-->
Recon is a sets reconsideration protocol to be used for synchronization of stream tips in ceramic network.
Stream Sets bundle a number of streams together so the ceramic node with a common interest in those streams can synchronize efficiently.
Once a set of streams is bundled into a stream set Recon can be used to sync ranges within those sets.


## Abstract
<!--A short (~200 word) description of the technical issue being addressed.-->
Stream Sets are bundles of streams that can be gossiped about as a group or in sub-ranges. In order to make ranges possible over a set of streams we need to define an order for the ranges to be over. e.g. `ceramic/<network>/<sep>/<EventID>` by assigning a recon key to each event we can define a range over the lexicographic sort of the sort keys. Now a node can advertise a have/want over a range `(first event ID, range_hash, last event ID)` If the node receiving the advertisements has the same set of events, then they will have the same hash and are in sync. If a node needs to advertise a new event it can send `(first_event_ID, hash, new_event_ID, hash, last_event_ID)` This tells any receiving node not only the new event ID but asserts all events in the range. If the receiving node has any additional or missing events in this range it is detected and can be synchronized.

New nodes interested in a range of a stream set can be discovered in two ways. First if a node receives a have/want advertisement from a node it can add that node to its list of peers. Second nodes should advertise their wants to other known nodes in the network.


## Motivation
<!--Motivation is critical for CIPs that want to change the Ceramic protocol. It should clearly explain why the existing protocol specification is inadequate to address the problem that the CIP solves. CIP submissions without sufficient motivation may be rejected outright.-->
Currently ceramic nodes broadcast updates to streams to every node in the network. This requires nodes to do work processing messages that they donâ€™t care about. At the same time if a node missed the broadcast, it would not detect the missing stream unless it hears a later update or performs historical sync via the CAS leger.

Recon could provide low to no overhead for nodes not synchronizing a given range, for ranges that a node is synchronizing have a high probability of getting the **latest** events from a stream shortly after any node has the events, and no need for remote connections at query time.

By pulling stream updates out of the main network channel into a stream set the nodes interested in those streams can synchronies with each other without putting load on uninterested nodes in the network. 

These Stream Sets will likely be defined by a set of models that should be included in the Stream Set. They could also be defined by a set of models and controllers but the large and changing number of controllers requires more thought and is not proposed here. They could even be defined be an arbitrary function that filters streams for membership in/out of the set. e.g. Failed some spam filter.

A stream also could potentially be included in more than one stream set if for example there are apps that want to sync every event across the entire network, they may want to form a stream set for all events that overlaps the narrower stream sets. We will start with one stream set that orders all the streams into a single stream set.

The introduction of stream sets should lower the burden of running a node that is not interested in a stream set. This is due to sending the synchronization connections only to nodes that advertise an interest in a range. Stream set synchronizations could speed up the process of syncing all the historical events that are within a range by by detecting and backfilling missed updates with every synchronization update.

A secondary goal of stream sets is to give a structure for sharding a stream set across ceramic nodes. By supporting an ability to synchronize only a sub-range of the stream set the burden of storing, indexing, and retrieving streams can be sharded among ceramic nodes.

e.g.
```
Node1: [
  "ceramic/<network>/<sep>/!", 
  "ceramic/<network>/<sep>/i"
]
Node2: [
  `ceramic/<network>/<sep>/i`,
  `ceramic/<network>/<sep>/~`
]
```
> note: The keys are base36 /`[0-9a-z]`/ so "`!`" is less then all keys and "`~`" is grater then all keys.

Ceramic nodes also need to have a way to find the other nodes interested in the stream set so that they can synchronize with them.
Recon will rely on gossiping about nodes existence and interests. When two nodes connect they can share lists of known nodes and their interests.

By gossiping about ranges of streams ceramic nodes that are arbitrarily out of sync can synchronize all events of mutual interest. Nodes that are in sync or nearly in sync can send each other very little bandwidth. Nodes can avoid sending stream event announcements to nodes that have no interest in the stream ranges.

## Specification
<!--The technical specification should describe the syntax and semantics of any new feature.-->

### **Stream Sets Ranges (Ordering)**
We need to choose a consistent ordering to give ranges meaning.
What order should we be sorting the EventIDs
A MID stream set could use a model ordering like
`
/ceramic/mainnet/3/model_controller/model/stream_controller/stream/height/event
`
We chose this over a time based order like
`/ceramic/mainnet/anchor_time/event_cid` since we didn't want to wait for events to be timestamped before syncing them.

#### **EventID sort order**
We need to translate to recon keys rather then using the EventID as the sort order because that would limit us the the StreamID as the top level grouping and we would end up with many small interest ranges rather then a few larger ranges.
> ðŸš§ The EventIDs are grouped by stream but then in random order
```tsx
// Current EventID
<EventID> ::= 
  <multibase-prefix 'k' base36>
    <multicodec 0xce streamid>
    <stream-type>
    <genesis-cid-bytes>
    <event-cid-bytes>
```
We could propose a new EventID that had the height in it but this is less flexible then allowing a spesific stream set to define its own `event -> recon key` mapping.
```tsx
// Proposed EventID
<EventID> ::=
  <multibase-prefix 'k' base36>
  <multicodec 0xce streamid>
  <stream-type varint>
  <inline-cid>

<inline-cid> ::=
  <cidv1 0x01>
  <multicodec-content-type 0x71 dag-cbor >
  <hash-function-code 0x00 identity>
  <dag-cbor-size-in-bytes varint >
  <dag-cbor-bytes [
    height: number, 
    event-cid: link
  ]>
```
[CIDv1](https://github.com/multiformats/cid)
```
<cidv1> ::=
  <multibase-prefix>
  <multicodec-cidv1>
  <multicodec-content-type>
  <multihash-content-address>
```
[Multihash](https://github.com/multiformats/multihash)
```
<multihash-content-address> ::=
  <varint hash function code>
  <varint digest size in bytes>
  <hash function output>
```
[multiformats/unsigned-varint](https://github.com/multiformats/unsigned-varint)

### **Ceramic node discovery and interest advertisements**
In order to discover peers with over lapping interests nodes must advertise there interest ranges to the network.

#### **Bootstrap**
In order to allow the Ceramic nodes to find each other we have a small set of bootstrap nodes with known locations.
When a node comes up it will call out the the bootstrap node to initialize the list of neighbors.
This list should be override-able by the environment for testing or closed network.

#### **Neighbor gossip**
Once a node has other nodes in its neighbors list it should periodically call each neighbor
and exchange neighbors list. Over time each node should learn about all other ceramic nodes. 

```ts
struct Neighbor {
    location: [multiaddr]
    last_seen: [us_since_epoch]
    latency: [ms_ping_rtt]
}

array Neighbors [Neighbor]
```

A node should probably limit the nodes it responds with to nodes that have be seen recently. e.g. Last 28 days.
When a new node comes up its existence will get gossiped around the network.

#### **Interests Gossip**
The Interests are ranges over the recon key space.
```
(start, (ahash, cid?), divider, (ahash, cid?), stop)
```

The messages are sent as CBOR
```ts
struct HaveMessage {
    k: [String], // keys must be 1 longer then hashes unless both are empty
    h: [String], // hashes must be 1 shorter then keys
    cid?: [CID], // optional CID of the sub-tree node if remote is a static blob store
}

struct Interests [
  {
    start: String,
    stop: String,
    have: HaveMessage,
  }
]
```

## Creating the recon key from a stream.

The key starts with the stream type identifier [varint](https://github.com/multiformats/unsigned-varint).

| name | code | description |
| --- | --- | --- |
| Tile | 0 | A stream type representing a Json document |
| CAIP-10 Link | 1 | Link blockchain accounts to DIDs |
| Model | 2 | Defines a group of documents in ComposeDB that share a schema |
| Model Instance Document | 3 | Represents a Json document in ComposeDB |
| UNLOADABLE | 4 | A stream that is not meant to be loaded |

The following bytes will depend on the stream type

- Tile
    - `0/<family 6 bytes>/<schema 6 bytes>/<StreamID full>/<Stream Height [varint](https://github.com/multiformats/unsigned-varint)>/<Event CID full>`
- CAIP-10 Link
    - `1/<StreamID full>/<Stream Height [varint](https://github.com/multiformats/unsigned-varint)>/<Event CID full>`
- Model
    - `2/<Model Controller 6 bytes>/<StreamID full>/<Stream Height [varint](https://github.com/multiformats/unsigned-varint)>/<Event CID full>`
- Model Instance Document
    - `3/<Model Controller 6 bytes>/<Model 6 bytes>/<Stream controller 6 bytes><StreamID full>/<Stream Height [varint](https://github.com/multiformats/unsigned-varint)>/<Event CID full>`
- UNLOADABLE
    - Unloadable documents have no structure just use the StreamID as the key
    - `4/<StreamID>`
    - `4/kh4q0ozorrgaq2mezktnrmdwleo1d` only unloadable StreamID in [current use](https://www.notion.so/7a8a163fb97047359c56b26b6146ce85)

The main risk in converting the events to keys is that we do want the keys to be grouped in ways that ceramic nodes are most likely to have large contiguous interests like the family for Tile Documents or the Modelâ€™s controller for Model Instance Documents. This is in tension with the desire to have short keys. Just the StreamID and event CID could be 124 characters add in an additional 6 char for `Model Controller`, `Model`, `Stream controller`, and 1 char for `Stream height` and `stream type` and we are talking about 142 chars for each stream. 

```python
In [0]: len(
 "3" + # Type: Model Instance Document
 "/JSwygB" + # Model Controller: did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt
 "/xp09sz" + # Model: kjzl6hvfrbw6c5sffjlmczg8nmbk8kwu9lmgiqfd9bxi7pxp14u674cuxp09szz
 "/JSwygB" + # Stream controller: did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt
 "/kjzl6kcym7w8y7hyovnujm2zbxa57z0z0yhmnlsx9qe4gtyurcbg6z2aw967s0d"+ # Stream ID
 "/1"+ # event height
 "/bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue" # Event CID
)
Out[0]: 148
```

With about a billon events per year as a target this is hundreds of gigabytes just for the keys without event accounting for the size of the events.

For the 6 char partition indicators we want to use 6 random chars. The beginning of the DID String or the StreamID is low entropy due to common prefixes. The last char is low entropy since it may have only a few bits in it depending on how the b36 lines up. We can take the penultimate 6 characters to get log2(36^6) â‰ˆ 31.02 bits of entropy. This could be used with `/` to make a key with no binary manipulations. It may be necessary to parse the DID URI in order to get the first path segment for the DID entropy and not read from the fragment.

### event height

The event height 0 for the init event in a stream.

The event height is 1 more than the max event height of the parent events for a non-init event.

**other option considered** 

The other option is to move all the fields to binary concatenate them and encode back to base36. This would be more complex but also the keys would be shorter.

## **Interactive Sync**
One or more interest range,
each containing one or more sub-tree associative hashs.
```
(EventID (sub-tree-ahash EventID)+ )+
```
We follow a synchronization protocol inspired by
"Range-Based Set Reconciliation and Authenticated Set Representations"
[[arXiv:2212.13567](https://doi.org/10.48550/arXiv.2212.13567)]

---
To keep the example small we substitute event keys like
`JSwygB/xp09sz/JSwygB/w967s0/0/bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue`
or
`JSwygB/xp09sz/JSwygB/w967s0/1/bagcqcerand3n6q246mfo2v7d6i7aacpxlfnfprhyid5rcnej2bawqnlnsogq`
with short strings like `ape` and `bee`.

![ring](assets/ring.png)

Example sync

```
you:  [ your initial keys ]
they: [ their initial keys ]
    -> ( request ) [ their keys after processing request]
    <- ( response ) [ your keys after processing response]
```

```
you:  [ape,eel,fox,gnu]
they: [bee,cat,doe,eel,fox,hog]
    -> (ape, h(eel,fox), gnu) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, h(bee,cat), doe, h(eel,fox), gnu, 0, hog) [ape,doe,eel,fox,gnu,hog]
    -> (ape, 0, doe, h(eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, 0, bee, 0, cat, h(doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    -> (ape, h(bee,cat,doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
    <- (ape, h(bee,cat,doe,eel,fox,gnu), hog) [ape,bee,cat,doe,eel,fox,gnu,hog]
```

You initiate a synchronization by sending a hash covering the entire range.
```
    -> (ape, h(eel,fox), gnu)
```

They don't have the same hash so they split the range near the middle at `"doe"`
They also have a key after your last key so they send `"hog"`
```
 <- (ape, h(bee,cat), doe, h(eel,fox), gnu, 0, hog)
```

You have nothing from `"ape"` to `"doe"` so send `0`.
Your hash from `"doe"` to `"gnu"` match.
Your hash from `"gnu"` to `"hog"` match.
You merge the match streak from `"doe"` to `"hog"`.
```
 -> (ape, 0, doe, h(eel,fox,gnu), hog)
```

You sent 0 from `"ape"` to `"doe"` so they send all keys in that range.
The rest match
```
<- (ape, 0, bee, 0, cat, h(doe,eel,fox,gnu), hog)
```

with the new key all ranges merge to `"ape"`, `"hog"`
they also merge to `"ape"`, `"hog"` nothing to send we are in sync
```
    -> (ape, h(bee,cat,doe,eel,fox,gnu), hog)
    <- (ape, h(bee,cat,doe,eel,fox,gnu), hog)
```

---
#### **Immutable object store**
A slight variant of the protocol can be used with a smart client and a dumb object store. Where all the computation takes place on the client and the storage just serves values for keys.

For syncing with an immutable object store we add CIDs to the intermediate nodes of a tree. This would be DAG-JSON or DAG-CBOR
```
(EventID (sub-tree-ahash, sub-tree-CID, EventID)+ )+
```
This protocol can be run against a object store. Instead of the remote deciding which segments to send a pre-built b-tree is stored in the object store. The client fetches the root of the b-tree. If the client has the same associative hash `sub-tree-ahash` for a subtree then it skips that subtree if it differs then it requests the b-tree node with `sub-tree-CID`. This continues recursively until all the events in the stored b-tree are synced down to the client.

The use of synchronization against an object store may improve the experience of syncing a brand new node or a node that has been down for an extended period of time. The long down node could start by synchronizing against a weekly snapshot and only after that sync with live nodes getting the bulk of the dataset from the object store.


### **Random Peer Synchronization Order**
When synchronizing we want to sync with nearby nodes with higher probability since this is lower cost but also sometimes with the far away nodes so that the whole network is synchronized. By tracking the latency of other nodes with overlapping interests we can choose a peer proportional to the overlapping interests and inversely proportional to the latency.
|| keys in overlapping interests || / latency

![Random gossip](assets/gossip.gif)

The number of nodes with the event will grow exponentially until it saturates the network.





## Rationale
<!--The rationale fleshes out the specification by describing what motivated the design and why particular design decisions were made. It should describe alternate designs that were considered and related work, e.g. how the feature is supported in other languages. The rationale may also provide evidence of consensus within the community, and should discuss important objections or concerns raised during discussion.-->
### Motivation for the design

Goals:

- Low to no overhead for peers not subscribed to the stream being queried.
- High probability of getting the ******latest****** tip (e.g. resilience to eclipse attacks etc.)
- Low overhead for the querying peer (e.g. no requirement to connect to and query a massive number of peers)

Ceramic has a model where each ceramic node with an interest in a stream stores the contents of the stream. This means when a stream is created or updated each ceramic node should eventually hear about the new event and update its stream tip store.

Currently this is being done by using a LibP2P PubSub channel. This works for low amounts of traffic while a node is up and listening to the channel. There are two main drawbacks of this method. One that if a node is down and misses an update it will not find out about the stream until the next update. Since the next update may never come nodes need to periodically run Historical Data Sync to discover events that it missed but are successfully anchored. Two that every node needs to process every update. Even if a node is only interested in a few low frequency models or controllers it still needs to be a large enough node to handle all updates and filter for the one that it is interested in.

The LibP2P PubSub channel is meant for data that has only a limited time of interest. It only remembers messages for a short time then expires them. If a node gets very behind in processing the incoming message queue it can re-broadcast messages after the other nodes have forgotten it causing the message to be redelivered to all nodes in the channel. Since the node is way behind it will see its own message later and re-re-broadcast it. This infinitely looping message can slow down other nodes that start re-broadcast messages leading to a network storm. A single slow, or malicious, node can harm the entire networkâ€™s ability to use the channel. Event update lost in the storm may not be picked up until Historical Data Sync runs.

By switching to a set reconciliation gossip protocol (Recon) we solve both problems.

One, if a node missed a stream update it will detect that the node it is synchronizing with has an event that it does not. A node could go down for years and when it rejoined the network it would just sync back to the current state.

Two, if a node is only interested in a subset of the streams, it can synchronize only the ranges that it cares about. A single node that is too slow to keep up with the number of streams it is interested in will start a synchronization that will never finish but will only cause load on the node that it is synchronizing with and not cause a network wide disruption.

---

### Alternate designs considered and related work

Hash graph stile gossip

Each node collects the event that occur on that node into an update block. Each time a node receives an update block from a different node it puts the CID of its previous block and the incoming block at the end of the current block to finish that update block and starts a new one. Then send the finished block to a random ceramic node. Since each block has the CID of two earlier blocks if can follow back to the beginning of time.

This was rejected since it did not allow for a node to follow a limited subset of the streams.

Predicated LibP2P PubSub channels

Can we change LibP2P PubSub to only send the events that a node cares about to limit the number of messages a node needs to process.

If we can use a TTE(Time To Expire) as part of the predicate we may be able to solve the network storm problem.

This was rejected because it does not solve the missed messages problem.

We still plan to revisit this in the future

#### **DHT for interest discovery**
In order to discover other ceramic nodes that have overlapping interest
a node could announce itself in the DHT as interested in a model.
the dht key would be derived by `separatorKey` and `separatorValue`
```
key =  are set, use `partition = hash(separatorKey | separatorValue)`
```

---
#### **Stream set sort order**
example lexicographical sort order
```
ceramic/<network>/<partition_key>/<EventID>
```
The `partition_key` would be defined for a specific stream set.

For the default stream set with all streams we could partition on`stream type`, `model`, `controller`, `time` , or something else using a partition key? We could even map multiple dimensions into a single partition key and then partition based on the calculated partition key.
![model controller time cube](assets/cube.png)

#### **What are the sorting orders that make sense?**
- Total time ordering of all anchored events in all streams.
    - `ceramic/<network>/<timeID>/<EventID>`
    - advantages: most divergence is in recent events
    - disadvantages:
      - hard to shard as most load is recent events,
      - hard to follow a stream,
      - hard to follow a controller 
- Sort by StreamID lexicographic
    - `ceramic/<network>/<StreamID>/<EventID>`
    - advantages: simple to shard to StreamID ranges
    - disadvantages: following a model or controller would be many ranges
- Sort by model/StreamID
    - `ceramic/<network>/<model>/<StreamID>/<event height><Event CID>`
    - advantages:
      - following a models is a single range
    - disadvantages:
      - following a controllers is a many ranges

- Sort by model_group/model/stream_group/StreamID/time/event
    - `ceramic/<network>/<model controller>/<model>/<stream controller>/<StreamID>/<event height>/<Event CID>`
    - advantages: models grouped by publisher, streams grouped by publisher, stream events in order
    - disadvantages: long keys

- Z-order (Hilbert Curve order)
    - `ceramic/<network>/< z(model, controller, time) >`
    - advantages: 
      - clustering by model, controller, and time
    - disadvantages:
      - complexity
      - all three dimensions have few ranges but none are single range.
    - notes:
      - [https://youtu.be/YLVkITvF6KU](https://youtu.be/YLVkITvF6KU)
      - [https://en.wikipedia.org/wiki/Z-order_(curve)](https://en.wikipedia.org/wiki/Z-order_(curve))
      - e.g.
        1. hash the model
        2. hash the controler
        3. timestamp
        3. interleave the bytes of the hash M00:C00:T00:M01:C01:T00:...
        4. cluster into chunks e.g. 64MiB files

example

partition_key = model_controller/model/stream_controller/stream/height/event
```
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6hvfrbw6c5sffjlmczg8nmbk8kwu9lmgiqfd9bxi7pxp14u674cuxp09szz/
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6kcym7w8y7hyovnujm2zbxa57z0z0yhmnlsx9qe4gtyurcbg6z2aw967s0d/
0/
bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue
```

```
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6hvfrbw6c5sffjlmczg8nmbk8kwu9lmgiqfd9bxi7pxp14u674cuxp09szz/
did:key:z6Mkq1r4LAsQTjCN7EBTnGf7DorL28aZ4eb6akcLwJSwygBt/
kjzl6kcym7w8y7hyovnujm2zbxa57z0z0yhmnlsx9qe4gtyurcbg6z2aw967s0d/
1/
bagcqcerand3n6q246mfo2v7d6i7aacpxlfnfprhyid5rcnej2bawqnlnsogq
```

this would be about 300 chars so we may want to abbreviate
model_controller/model/stream_controller/stream/height/event
`JSwygB/xp09sz/JSwygB/w967s0/0/bafyreidx27tvivoh4hre4xrjnqprntsbmvsoujydcr5cinu4b2exqjeeue`
`JSwygB/xp09sz/JSwygB/w967s0/1/bagcqcerand3n6q246mfo2v7d6i7aacpxlfnfprhyid5rcnej2bawqnlnsogq`

Model_controller keeps model published by the same model creator together.
Model keeps all the documents of a model together.
Stream_controller keeps streams published by the same stream creator together.
StreamID keeps the updates to a stream together.
Height keeps the updates to a stream in order.

Since interests are expressed as interest ranges a ceramic node interested in a set of models relevant to an application
will be able to subscribe with a small number of ranges. The number of ranges will be on the order of the number of
model_controllers they are interested in. If application developers publish all the models for their application using
a single controller then the models will be efficient to sync as a group.

## Backwards Compatibility
<!--All CIPs that introduce backwards incompatibilities must include a section describing these incompatibilities and their severity. The CIP must explain how the author proposes to deal with these incompatibilities. CIP submissions without a sufficient backwards compatibility section may be rejected outright.-->
This protocol should run in parallel with the libp2p pubsub channel for tip synchronization with out a problem.

<!--
## Implementation
<!--The implementations must be completed before any CIP is given status "Final", but it need not be completed before the CIP is accepted.-- >
Implementation goes here.
-->


## Security Considerations
<!--All CIPs must contain a section that discusses the security implications/considerations relevant to the proposed change. Include information that might be important for security discussions, surfaces risks and can be used throughout the life cycle of the proposal. E.g. include security-relevant design decisions, concerns, important discussions, implementation-specific guidance and pitfalls, an outline of threats and risks and how they are being addressed. CIP submissions missing the "Security Considerations" section will be rejected. An CIP cannot proceed to status "Final" without a Security Considerations discussion deemed sufficient by the reviewers.-->
The associative hash functions are only secure if the node is asked to produce the EventIDs that hash to the SHAs that make up the SumOfShas associative hash.

Recon works when you want to synchronize event that are contiguous in the key ordering.
If a spammer is creating junk events each with there own controller did they will be spread uniformly throw the model.
This implies that if we want to exclude the spam events there will be many holes and the representation will have many ranges. 

## Appendix
### **B#tree (B hash trees)**
e.g. [MST](https://hal.inria.fr/hal-02303490/document) / [Prolly Trees](https://docs.dolthub.com/architecture/storage-engine/prolly-tree)

Advantage:
  - Can be sharded to split among nodes.
  - Can have independent tree structure, fanout, balancing, or leveling.

Disadvantage:
  - A stream set must agree on a sorting order of events.

![b hash tree](assets/b_hash_tree.png)

The B#tree (Bee-Hash-Tree) is a form of B-tree
where the links are hashes rather than pointers.

1. Pointers are CIDs (hashes)
2. iNode is functionally **dependent** on the nodes that **the node points to**.
3. iNode is functionally **independent** of the nodes that **point to the node**.
4. Keys and subtrees are in sorted order within an iNode
5. Level is functionally dependent set of keys, and optionally values


![merkle search tree](assets/merkle_search_tree.png)

The MST uses only the hash of the key to determine the level of the tree where that key is stored.
This will lead to probabilistically log(n) depth and log(n) keys in the per node.
The Prolly tree uses a running hash of the keys from left to right to determine level of the tree.
Sliding a fixed-size window through it, one byte at a time.
This enables the Prolly Tree to better calibrate the node size variance.
e.g. As the node size grows increase the probability of a boundary. 
This will reduce the odds of small nodes and large nodes
in favor of target size nodes.


### **Associative Hash Function (SumOfSha256s)**
If we want to calculate the hash of a range of
EventIDs without needing to visit all of the events
we can use an associative hash function where the grouping
of EventIDs is not relevant to the final hash.

```
h((A, B), C) = h(A, (B, C))
```

e.g. use `sum(sha256(EventID) for EventID in Stream Set)`
each node in the tree can store the associative hash of each
sub tree along with its max and min we only need to recurse into that sub-tree if the range end is in the sub-tree. Since a range has only two ends, start and stop, we can calculate the associative hash by visiting only twice the depth of the tree number of nodes `2*log_b(n)`
where b is the fanout and n is the number of EventIDs.

#### ahash
To get the ahash of a set we start by using SHA256 two convert the set elements to 32 byte hashs.
Next each of the 32 byte hashes are treated as an array of 8 unsigned little endian 32 bit integers.

To add to ahashs we use piecewise addition with all the additions here mod `2^32`. `C = A + B`
```
c[0] = a[0] + b[0]; c[1] = a[1] + b[1]; c[2] = a[2] + b[2]; c[3] = a[3] + b[3];
c[4] = a[4] + b[4]; c[5] = a[5] + b[5]; c[6] = a[6] + b[6]; c[7] = a[7] + b[7];
```
Due to the associativity it doesn't matter the order in witch you add the elements of the set.
This can be done in a loop with an accumulator or as a tree.
If you have a large set distributed across many nodes the hasher can hash and add all element locally and
then send the hashes to one node for final combination.

Little endian 32 bit integers are chosen since x86 and arm CPUs use little endian unsigned integers.
u32 was chosen since it will fit in a JS number and can be calculated in js without on reliance.
big number libraries.

Treating the hash as u8 was rejected since it is less performant and using the xor as the associative add
was rejected since having a value twice will look the same as not havening that element at all.

A b-tree with fanout 3:
![fanout3](assets/b_hash_tree_1.png)

A b-tree with fanout 2:
![fanout2](assets/b_hash_tree_2.png)
## Copyright
Copyright and related rights waived via [CC0](https://creativecommons.org/publicdomain/zero/1.0/).
